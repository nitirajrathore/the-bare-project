{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "MODEL=\"llama3.2:3b\"\n",
    "PROVIDER=\"ollama\"\n",
    "URL=\"http://213.180.0.49:47934\"\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(MODEL, model_provider=PROVIDER, base_url=URL  # Change if running on a remote server\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44303/3468921569.py:4: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=MODEL, base_url=URL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.168779134750366, 0.6916805505752563, -1.2653716802597046, -0.050212688744068146, -0.6538697481155396]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Initialize Ollama embeddings with a specific model (e.g., 'mistral' or 'llama2')\n",
    "embeddings = OllamaEmbeddings(model=MODEL, base_url=URL)\n",
    "\n",
    "# Text to embed\n",
    "text = \"LangChain makes working with LLMs easy.\"\n",
    "\n",
    "# Generate embedding\n",
    "embedding_vector = embeddings.embed_query(text)\n",
    "\n",
    "# Print the embedding vector\n",
    "print(embedding_vector[:5])  # Printing first 5 values for readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm just a language model, so I don't have feelings or emotions like humans do. However, I'm functioning properly and ready to assist you with any questions or tasks you may have! How can I help you today?\", additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-03-10T05:38:05.252621842Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4108237403, 'load_duration': 3216329019, 'prompt_eval_count': 31, 'prompt_eval_duration': 352000000, 'eval_count': 47, 'eval_duration': 535000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-89275532-4251-4ad3-828f-ce390b12467c-0', usage_metadata={'input_tokens': 31, 'output_tokens': 47, 'total_tokens': 78})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "llm.invoke(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'init_embedding' from 'langchain_core.embeddings' (/home/nitiraj/praroopai/mywork/the-bare-project/langchain/rag-app/.venv/lib/python3.12/site-packages/langchain_core/embeddings/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_embedding\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize Ollama embeddings in a provider-agnostic way\u001b[39;00m\n\u001b[1;32m      4\u001b[0m ollama_embeddings \u001b[38;5;241m=\u001b[39m init_embedding(PROVIDER, model\u001b[38;5;241m=\u001b[39mMODEL, base_url\u001b[38;5;241m=\u001b[39mURL)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'init_embedding' from 'langchain_core.embeddings' (/home/nitiraj/praroopai/mywork/the-bare-project/langchain/rag-app/.venv/lib/python3.12/site-packages/langchain_core/embeddings/__init__.py)"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
